{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum machine learning in Amazon Braket Hybrid Jobs\n",
    "\n",
    "This notebook demonstrates  training a parameterized quantum circuit Born machine (QCBM) for an unsupervised generative modelling task.\n",
    "\n",
    "\n",
    "## Quantum Circuit Born Machine \n",
    "\n",
    "Quantum circuits are a natural fit for generative modelling because they are inherently probabilistic; the wavefunction encodes a probability according to the Born rule:\n",
    "\n",
    "$$p(x)=|\\langle x|\\psi\\rangle|^2$$\n",
    "\n",
    "In quantum mechanics, we do not have access to $p(x)$ directly, but we can efficiently sample using projective measurements [1]. This is an implicit generative model similar to generative adversarial networks (GANs). Quantum circuits allow fast sampling from a high-dimension distribution, and have large expressive power. \n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\n",
    "[1] Benedetti, Marcello, Delfina Garcia-Pintos, Oscar Perdomo, Vicente Leyton-Ortega, Yunseong Nam, and Alejandro Perdomo-Ortiz. “A Generative Modeling Approach for Benchmarking and Training Shallow Quantum Circuits.” Npj Quantum Information 5, no. 1 (May 27, 2019): 1–9. https://doi.org/10.1038/s41534-019-0157-8.\n",
    "\n",
    "[2] Liu, Jin-Guo, and Lei Wang. “Differentiable Learning of Quantum Circuit Born Machine.” Physical Review A 98, no. 6 (December 19, 2018): 062324. https://doi.org/10.1103/PhysRevA.98.062324.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from braket.devices import LocalSimulator\n",
    "\n",
    "from braket.experimental.algorithms.quantum_circuit_born_machine import QCBM, mmd_loss\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 3\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(1)\n",
    "\n",
    "device = LocalSimulator()\n",
    "\n",
    "qcbm = QCBM(device, n_qubits, n_layers, data)\n",
    "\n",
    "print(qcbm.parametric_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "As an example, we consider the toy example of learning a mixture of Gaussian distributions. We set a numpy random seed to produce the same data each time, but try experimenting with the number of peaks and number of qubits to produce harder or easier data sets. For this example, the target distribution $p(x)$ is a Gaussian on 5 bits (so $2^3$ possible values), with peaks at $\\mu_1=1$ and $\\mu_2=10$, with standard deviations $\\sigma_1=1$, $\\sigma_2 = 2$. We generate and plot the data as a probability density function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(n_qubits, mu, sigma=1):\n",
    "    x = np.arange(2**n_qubits)\n",
    "    gaussian = 1.0 / np.sqrt(2 * np.pi * sigma**2) * np.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "    return gaussian / sum(gaussian)\n",
    "\n",
    "\n",
    "data = gaussian(n_qubits, mu=1, sigma=1) + gaussian(n_qubits, mu=10, sigma=2)\n",
    "data = data / sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [format(i, \"b\").zfill(n_qubits) for i in range(len(data))]\n",
    "plt.bar(range(2**n_qubits), data)\n",
    "plt.xticks(list(range(len(data))), labels, rotation=\"vertical\", size=12)\n",
    "plt.xlabel(\"Sample\", size=20)\n",
    "plt.ylabel(\"Probability\", size=20)\n",
    "plt.title(\"Two-peak Gaussian distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "\n",
    "Next, we set the hyperparameters for our training job. To keep it simple, we only consider the following hyperparameters: number of qubits `n_qubits`, number of layers in the QCBM `n_layers`, and the number of iterations in the optimization algorithm.\n",
    "\n",
    "The number of layers determines how many parameters are in the quantum circuit. For the QCBM, we need `n_params = 3 * n_layers * n_qubits`.\n",
    "\n",
    "Note that the hyperparameters are defined in a Python dictionary of strings. Within the `qcbm_job.py` script, we will need to convert from strings back into integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare hyperparameters for QCBM\n",
    "n_iterations = 5\n",
    "n_layers = 3\n",
    "\n",
    "init_params = np.random.rand(3 * n_layers * n_qubits)\n",
    "\n",
    "qcbm = QCBM(device, n_qubits, n_layers, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing circuit\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "history = []\n",
    "\n",
    "\n",
    "def callback(x):\n",
    "    loss = mmd_loss(qcbm.get_probabilities(x), data)\n",
    "    history.append(loss)\n",
    "\n",
    "\n",
    "result = minimize(\n",
    "    lambda x: mmd_loss(qcbm.get_probabilities(x), data),\n",
    "    x0=init_params,\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=lambda x: qcbm.gradient(x),\n",
    "    options={\"maxiter\": n_iterations},\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Our first quantum machine learning job finished! Now let’s look at the convergence of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history, \"-o\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Convergence of the loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the original probability distribution, and the QCBM prediction probability\n",
    "\n",
    "qcbm_probs = qcbm.get_probabilities(result[\"x\"])\n",
    "\n",
    "plt.bar(range(2**n_qubits), data, label=\"target probability\", alpha=1, color=\"tab:blue\")\n",
    "plt.bar(range(2**n_qubits), qcbm_probs, label=\"QCBM probability\", alpha=0.8, color=\"tab:green\")\n",
    "plt.xticks(list(range(len(data))), labels, rotation=\"vertical\", size=12)\n",
    "plt.yticks(size=12)\n",
    "\n",
    "plt.xlabel(\"Sample\", size=20)\n",
    "plt.ylabel(\"Probability\", size=20)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As expected, the QCBM probability distribution closes matches the target distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('braket')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5904cb9a2089448a2e1aeb5d493d227c9de33e591d7c07e4016fb81e71061a5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
